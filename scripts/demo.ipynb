{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a5158fd-597b-4e09-a39a-a4ed52d59d66",
   "metadata": {},
   "source": [
    "# Learning with few labels\n",
    "\n",
    "In this notebook we try to see the effect of two things:\n",
    "1) Semi-supervised learning\n",
    "2) Label propagation\n",
    "\n",
    "We work on the DBPedia dataset, which is a text classification task with 14 classes. We formulate this as a multi-label problem, meaning the last classification layer will have 14 dimensions, and the datapoint labels are one-hot encoded.\n",
    "\n",
    "We split the available training data (560k) to 100 labeled training, 9900 labeled validation, and 550k \"assumingly\" unlabeled sets. We look at loss and accuracy (selecting the output with the highest logit value).\n",
    "\n",
    "We try to compare four training scenarios:\n",
    "1) A classifier which only uses labeled data;\n",
    "2) A classifier using labeled data on top of a base model trained on unlabeled data using MLM training;\n",
    "3) A classifier using labeled data, but also benefitting from label propagation on unlabeled data;\n",
    "4) All together: A classifier using labeled data on top of a base model trained on unlabeled data using MLM training, and also benefitting from label propagation on unlabeled data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180e6dbe-77c6-4270-8052-dc656bc5a499",
   "metadata": {},
   "source": [
    "## Importing requirements\n",
    "You can also add an extra cell to install the needed requirements:\n",
    "```\n",
    "!pip install torch\n",
    "!pip install scikit-learn\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8aa1587-64cc-434d-b0d1-4dacae441e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from datasets import list_datasets, load_dataset, concatenate_datasets\n",
    "from transformers import AutoModelForSequenceClassification, AutoModelForMaskedLM, AutoTokenizer, AutoConfig, AutoModel\n",
    "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments, EvalPrediction\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from tqdm import tqdm\n",
    "from sklearn.semi_supervised import LabelPropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaf796e-9c32-43a4-8bce-5692c0c03616",
   "metadata": {},
   "source": [
    "## Loading DBPedia dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd7c612-4f12-4a36-8c3e-fe1485d772f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset('dbpedia_14', split='train')\n",
    "dataset = load_dataset('nlu_evaluation_data', split='train')\n",
    "print(dataset)\n",
    "print(dataset.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa63f40-b22c-4496-a4a3-d92cd33be68f",
   "metadata": {},
   "source": [
    "## Creating a tokenizer\n",
    "We use BERT base uncased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72f39e5f-a674-4510-b769-c1cbfe57cc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24af3634-cb09-41d0-bb8f-215e5af66ebf",
   "metadata": {},
   "source": [
    "## Preprocessing the dataset\n",
    "We process the dataset by tokenizin the text and one-hot encoding the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d62762c-ea71-4c65-8ab2-c2a9201c93a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset.map(lambda examples: tokenizer(examples['content'], truncation=True, max_length=256, padding='max_length'), batched=True)\n",
    "dataset = dataset.map(lambda examples: tokenizer(examples['text'], truncation=True, max_length=256, padding='max_length'), batched=True)\n",
    "dataset = dataset.map(lambda examples: {'labels': [1.0 if i == examples['label'] else 0.0 for i in range(dataset.features['label'].num_classes)]}, batched=False)\n",
    "dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b8ecf9-a9a6-49ea-9cbb-ff731b590f7a",
   "metadata": {},
   "source": [
    "## Creating splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36f8959-0deb-4986-a0e0-843f3cf68481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset, valset = torch.utils.data.random_split(dataset, [550100, 9900], generator=torch.Generator().manual_seed(42))\n",
    "# trainset_labeled, trainset_unlabeled = torch.utils.data.random_split(trainset, [100, 550000], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "trainset, valset = torch.utils.data.random_split(dataset, [20000, 5715], generator=torch.Generator().manual_seed(42))\n",
    "trainset_labeled, trainset_unlabeled = torch.utils.data.random_split(trainset, [5000, 15000], generator=torch.Generator().manual_seed(42))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f22b9b-2003-491e-9d94-f523cee43bf1",
   "metadata": {},
   "source": [
    "## Creating the MLM trained model\n",
    "We use the \"unlabeled\" data to train a model using Masked Language Modeling (MLM). We will use this base model later in some experiments. The training take quite a few hours of GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2253979-4f98-42ed-85ea-cf8ec465525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_model = AutoModelForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "mlm_data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "mlm_training_args = TrainingArguments(\n",
    "    output_dir=\"./models/nlu_evaluation_data/mlm\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=5,\n",
    "    prediction_loss_only=False,\n",
    "    load_best_model_at_end=False,\n",
    "    logging_first_step=True,\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "mlm_trainer = Trainer(\n",
    "    model=mlm_model,\n",
    "    args=mlm_training_args,\n",
    "    data_collator=mlm_data_collator,\n",
    "    train_dataset=trainset_unlabeled,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3e019c-a99b-40a7-b06d-174034f34457",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbbc54c-705f-4d67-80dd-49f8585c48d7",
   "metadata": {},
   "source": [
    "## Training a classifier only on labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8da937-03d3-4c80-9e9c-364c6958c3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_config = AutoConfig.from_pretrained('bert-base-uncased', num_labels=dataset.features['label'].num_classes)\n",
    "classifier_model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', config=classifier_config)\n",
    "\n",
    "# If you want to freeze BERT weights\n",
    "for param in classifier_model.bert.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    accuracy = np.mean(np.argmax(p.predictions, axis=1) == np.argmax(p.label_ids, axis=1))\n",
    "    return {'accuracy': accuracy}\n",
    "    \n",
    "classifier_training_args = TrainingArguments(\n",
    "    output_dir='./models/nlu_evaluation_data/classifier',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=64,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=5,\n",
    "    prediction_loss_only=False,\n",
    "    load_best_model_at_end=False,\n",
    "    logging_first_step=True,\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy='epoch'\n",
    ")\n",
    "\n",
    "classifier_trainer = Trainer(\n",
    "    model=classifier_model,\n",
    "    args=classifier_training_args,\n",
    "    train_dataset=trainset_labeled,\n",
    "    eval_dataset=valset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a2b2df-42ac-4a3d-a162-de59515b4664",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3c82a1-36d4-4ac8-bbc3-2507587c44b6",
   "metadata": {},
   "source": [
    "## Training a classifier on labeled data on top of MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35982d2a-b386-4f28-836a-bb90dde04ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_mlm_config = AutoConfig.from_pretrained('bert-base-uncased', num_labels=dataset.features['label'].num_classes)\n",
    "classifier_mlm_model = AutoModelForSequenceClassification.from_pretrained('./models/nlu_evaluation_data/mlm/checkpoint-9375', config=classifier_mlm_config)\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    accuracy = np.mean(np.argmax(p.predictions, axis=1) == np.argmax(p.label_ids, axis=1))\n",
    "    return {'accuracy': accuracy}\n",
    "    \n",
    "classifier_mlm_training_args = TrainingArguments(\n",
    "    output_dir='./models/nlu_evaluation_data/classifier_on_mlm',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=64,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=5,\n",
    "    prediction_loss_only=False,\n",
    "    load_best_model_at_end=False,\n",
    "    logging_first_step=True,\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy='epoch'\n",
    ")\n",
    "\n",
    "classifier_mlm_trainer = Trainer(\n",
    "    model=classifier_mlm_model,\n",
    "    args=classifier_mlm_training_args,\n",
    "    train_dataset=trainset_labeled,\n",
    "    eval_dataset=valset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a0eeb9-aa3d-4e8b-99b5-de7530bbc858",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_mlm_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c17e7dc-8e30-44e7-8983-cdcb762475a7",
   "metadata": {},
   "source": [
    "## A simple label agreement model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d791bed-a3a7-4fa7-b87d-fce905d54a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForAgreement(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        bert_config = AutoConfig.from_pretrained('bert-base-uncased')\n",
    "        self.bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.linear = torch.nn.Linear(bert_config.hidden_size, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, dict_1, dict_2):\n",
    "        labels = (1.0 - torch.sum(dict_1['labels'] * dict_2['labels'], dim=1)).unsqueeze(1)\n",
    "        bert_outputs_1 = self.bert(input_ids=dict_1['input_ids'], attention_mask=dict_1['attention_mask'], token_type_ids=dict_1['token_type_ids'])\n",
    "        bert_outputs_2 = self.bert(input_ids=dict_2['input_ids'], attention_mask=dict_2['attention_mask'], token_type_ids=dict_2['token_type_ids'])\n",
    "        aggregated = (bert_outputs_1.pooler_output - bert_outputs_2.pooler_output) ** 2\n",
    "        logits = self.linear(self.dropout(aggregated))\n",
    "        output = self.sigmoid(logits)\n",
    "        loss = torch.mean((output - labels) ** 2)\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "        )\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForPairing:\n",
    "    def __call__(self, batch):\n",
    "        # First rename the keys to *_1\n",
    "        for k in batch.keys():\n",
    "            batch[k + '_1'] = batch.pop(k)\n",
    "        # For each item create a positive and negative pair\n",
    "        # 'input_ids', 'token_type_ids', 'attention_mask', 'labels'\n",
    "        for index, label in enumerate(batch['labels_1']):\n",
    "            indices_without_this = list(range(0, len(batch['labels_1'])))\n",
    "            indices_without_this.remove(index) \n",
    "            labels_without_this = batch['labels_1'][indices_without_this]\n",
    "            input_ids_without_this = batch['input_ids_1'][indices_without_this]\n",
    "            token_type_ids_without_this = batch['token_type_ids_1'][indices_without_this]\n",
    "            attention_mask_without_this = batch['attention_mask_1'][indices_without_this]\n",
    "            # Negative pair\n",
    "            is_other_label = torch.any(labels_without_this - label, dim=1)\n",
    "            random_other_index = random.choice(torch.where(is_other_label)[0])\n",
    "            \n",
    "    \n",
    "agreement_model = BertForAgreement()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e90f567-9839-4279-9f00-61fff9629a58",
   "metadata": {},
   "source": [
    "## Graph Agreement Model (GAM) based training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d458c66-3f97-44d5-94d3-bb8f36a612a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop\n",
    "\n",
    "# Training the agreement G model using L\n",
    "\n",
    "# Training classification model F using L and predictions of G on U\n",
    "\n",
    "# Extend L using the most (M = 200) confident predictions of F on U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e901f2bf-0510-4d8e-a92a-39b49141627b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_up(batch, match):\n",
    "    result = {\n",
    "        'text_other': [],\n",
    "        'match': [],\n",
    "    }\n",
    "    label_to_indices = defaultdict(list)\n",
    "    for index, label in enumerate(batch['label']):\n",
    "        label_to_indices[label].append(index)\n",
    "    for label, text in zip(batch['label'], batch['text']):\n",
    "        if match == 'positive':\n",
    "            random_positive_index = random.choice(label_to_indices[label])\n",
    "            result['match'].append(1)\n",
    "            result['text_other'].append(batch['text'][random_positive_index])\n",
    "        if match == 'negative':\n",
    "            labels_wihtout_this = [label for label in label_to_indices.keys() if label_to_indices[label]]\n",
    "            labels_wihtout_this.remove(label)\n",
    "            # print(labels_wihtout_this)\n",
    "            if not labels_wihtout_this:\n",
    "                print(batch)\n",
    "            random_negative_label = random.choice(labels_wihtout_this)\n",
    "            random_negative_index = random.choice(label_to_indices[random_negative_label])\n",
    "            result['match'].append(0)\n",
    "            result['text_other'].append(batch['text'][random_negative_index])            \n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f39370e0-142b-4af7-b6a4-f108e8c5f98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset nlu_evaluation_data (/home/beast/.cache/huggingface/datasets/nlu_evaluation_data/default/1.1.0/0416a5876d8240bd571f2bc2ad421cf6e6e88d938f8dcb5fd87b5af6033d6282)\n",
      "Loading cached processed dataset at /home/beast/.cache/huggingface/datasets/nlu_evaluation_data/default/1.1.0/0416a5876d8240bd571f2bc2ad421cf6e6e88d938f8dcb5fd87b5af6033d6282/cache-ee9c96cf5cb16303.arrow\n"
     ]
    }
   ],
   "source": [
    "paired_dataset_positive = load_dataset('nlu_evaluation_data', split='train')\n",
    "paired_dataset_positive = paired_dataset_positive.map(lambda examples: pair_up(examples, match='positive'), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a89e638f-cc7d-4895-8e19-d062b45a023a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset nlu_evaluation_data (/home/beast/.cache/huggingface/datasets/nlu_evaluation_data/default/1.1.0/0416a5876d8240bd571f2bc2ad421cf6e6e88d938f8dcb5fd87b5af6033d6282)\n",
      "Loading cached shuffled indices for dataset at /home/beast/.cache/huggingface/datasets/nlu_evaluation_data/default/1.1.0/0416a5876d8240bd571f2bc2ad421cf6e6e88d938f8dcb5fd87b5af6033d6282/cache-21da13cc2f2c4679.arrow\n",
      "Loading cached processed dataset at /home/beast/.cache/huggingface/datasets/nlu_evaluation_data/default/1.1.0/0416a5876d8240bd571f2bc2ad421cf6e6e88d938f8dcb5fd87b5af6033d6282/cache-6f7a5b5440e912b0.arrow\n"
     ]
    }
   ],
   "source": [
    "paired_dataset_negative = load_dataset('nlu_evaluation_data', split='train')\n",
    "paired_dataset_negative = paired_dataset_negative.shuffle(seed=42)\n",
    "paired_dataset_negative = paired_dataset_negative.map(lambda examples: pair_up(examples, match='negative'), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c7852f3-5ddd-4c39-a266-4d91eeb619d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_dataset = concatenate_datasets([paired_dataset_positive, paired_dataset_negative])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d406c0d5-f7be-463f-a25c-462ace1c2d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'match', 'scenario', 'text', 'text_other'],\n",
       "    num_rows: 51430\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paired_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04d14abc-1855-4e2c-922d-880fb1fc587d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/beast/.cache/huggingface/datasets/nlu_evaluation_data/default/1.1.0/0416a5876d8240bd571f2bc2ad421cf6e6e88d938f8dcb5fd87b5af6033d6282/cache-dbc19ddc7135b376.arrow\n",
      "Loading cached processed dataset at /home/beast/.cache/huggingface/datasets/nlu_evaluation_data/default/1.1.0/0416a5876d8240bd571f2bc2ad421cf6e6e88d938f8dcb5fd87b5af6033d6282/cache-22c3507862b89017.arrow\n"
     ]
    }
   ],
   "source": [
    "paired_dataset = paired_dataset.map(lambda examples: tokenizer(examples['text'], examples['text_other'], truncation=True, max_length=256, padding='max_length'), batched=True)\n",
    "paired_dataset = paired_dataset.map(lambda examples: {'labels': examples['match']}, batched=True)\n",
    "paired_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c54ad764-68d3-4103-a495-c5b0fd39cdf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'label', 'labels', 'match', 'scenario', 'text', 'text_other', 'token_type_ids'],\n",
       "    num_rows: 51430\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paired_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfc41b87-2cc4-48b2-b4bd-215503feb7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_trainset, paired_valset = torch.utils.data.random_split(paired_dataset, [40000, 11430], generator=torch.Generator().manual_seed(42))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62075450-380b-4f9b-b7fc-07cada03d870",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='704' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 704/2500 05:37 < 14:22, 2.08 it/s, Epoch 0.28/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# If you want to freeze BERT weights\n",
    "# for param in model.bert.parameters():\n",
    "#     param.requires_grad = False\n",
    "    \n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    accuracy = np.mean(np.argmax(p.predictions, axis=1) == p.label_ids)\n",
    "    return {'accuracy': accuracy}\n",
    "    \n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./models/nlu_evaluation_data/test',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=5,\n",
    "    prediction_loss_only=False,\n",
    "    load_best_model_at_end=False,\n",
    "    logging_first_step=True,\n",
    "    logging_steps=1000,\n",
    "    evaluation_strategy='steps'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=paired_trainset,\n",
    "    eval_dataset=paired_valset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb304b07-a1c4-426f-963d-b293c648962b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
