{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a5158fd-597b-4e09-a39a-a4ed52d59d66",
   "metadata": {},
   "source": [
    "# Learning with few labels\n",
    "\n",
    "In this notebook we try to see the effect of two things:\n",
    "1) Semi-supervised learning\n",
    "2) Label propagation\n",
    "\n",
    "We work on the DBPedia dataset, which is a text classification task with 14 classes. We formulate this as a multi-label problem, meaning the last classification layer will have 14 dimensions, and the datapoint labels are one-hot encoded.\n",
    "\n",
    "We split the available training data (560k) to 100 labeled training, 9900 labeled validation, and 550k \"assumingly\" unlabeled sets. We look at loss and accuracy (selecting the output with the highest logit value).\n",
    "\n",
    "We try to compare four training scenarios:\n",
    "1) A classifier which only uses labeled data;\n",
    "2) A classifier using labeled data on top of a base model trained on unlabeled data using MLM training;\n",
    "3) A classifier using labeled data, but also benefitting from label propagation on unlabeled data;\n",
    "4) All together: A classifier using labeled data on top of a base model trained on unlabeled data using MLM training, and also benefitting from label propagation on unlabeled data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180e6dbe-77c6-4270-8052-dc656bc5a499",
   "metadata": {},
   "source": [
    "## Importing requirements\n",
    "You can also add an extra cell to install the needed requirements:\n",
    "```\n",
    "!pip install torch\n",
    "!pip install scikit-learn\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8aa1587-64cc-434d-b0d1-4dacae441e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import pathlib\n",
    "import elasticsearch\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from datasets import list_datasets, load_dataset, concatenate_datasets\n",
    "from transformers import AutoModelForSequenceClassification, AutoModelForMaskedLM, AutoTokenizer, AutoConfig, AutoModel\n",
    "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments, EvalPrediction\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from tqdm import tqdm\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaf796e-9c32-43a4-8bce-5692c0c03616",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cd7c612-4f12-4a36-8c3e-fe1485d772f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset nlu_evaluation_data (/home/beast/.cache/huggingface/datasets/nlu_evaluation_data/default/1.1.0/0416a5876d8240bd571f2bc2ad421cf6e6e88d938f8dcb5fd87b5af6033d6282)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('nlu_evaluation_data', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da045f33-a2de-4b85-aa67-f4995e64074f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/beast/.cache/huggingface/datasets/nlu_evaluation_data/default/1.1.0/0416a5876d8240bd571f2bc2ad421cf6e6e88d938f8dcb5fd87b5af6033d6282/cache-37939fecc47e5496.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['item_number', 'label', 'scenario', 'text'],\n",
      "    num_rows: 25715\n",
      "})\n",
      "{'item_number': Value(dtype='int64', id=None), 'label': ClassLabel(num_classes=68, names=['alarm_query', 'alarm_remove', 'alarm_set', 'audio_volume_down', 'audio_volume_mute', 'audio_volume_other', 'audio_volume_up', 'calendar_query', 'calendar_remove', 'calendar_set', 'cooking_query', 'cooking_recipe', 'datetime_convert', 'datetime_query', 'email_addcontact', 'email_query', 'email_querycontact', 'email_sendemail', 'general_affirm', 'general_commandstop', 'general_confirm', 'general_dontcare', 'general_explain', 'general_greet', 'general_joke', 'general_negate', 'general_praise', 'general_quirky', 'general_repeat', 'iot_cleaning', 'iot_coffee', 'iot_hue_lightchange', 'iot_hue_lightdim', 'iot_hue_lightoff', 'iot_hue_lighton', 'iot_hue_lightup', 'iot_wemo_off', 'iot_wemo_on', 'lists_createoradd', 'lists_query', 'lists_remove', 'music_dislikeness', 'music_likeness', 'music_query', 'music_settings', 'news_query', 'play_audiobook', 'play_game', 'play_music', 'play_podcasts', 'play_radio', 'qa_currency', 'qa_definition', 'qa_factoid', 'qa_maths', 'qa_stock', 'recommendation_events', 'recommendation_locations', 'recommendation_movies', 'social_post', 'social_query', 'takeaway_order', 'takeaway_query', 'transport_query', 'transport_taxi', 'transport_ticket', 'transport_traffic', 'weather_query'], names_file=None, id=None), 'scenario': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "# We add the item index as it is useful later\n",
    "dataset = dataset.map(lambda examples, idx: {'item_number': idx}, with_indices=True)\n",
    "print(dataset)\n",
    "print(dataset.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa63f40-b22c-4496-a4a3-d92cd33be68f",
   "metadata": {},
   "source": [
    "## Creating a tokenizer\n",
    "We use BERT base uncased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72f39e5f-a674-4510-b769-c1cbfe57cc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b8ecf9-a9a6-49ea-9cbb-ff731b590f7a",
   "metadata": {},
   "source": [
    "## Creating splits and sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d36f8959-0deb-4986-a0e0-843f3cf68481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/beast/.cache/huggingface/datasets/nlu_evaluation_data/default/1.1.0/0416a5876d8240bd571f2bc2ad421cf6e6e88d938f8dcb5fd87b5af6033d6282/cache-0ded8057637aabee.arrow and /home/beast/.cache/huggingface/datasets/nlu_evaluation_data/default/1.1.0/0416a5876d8240bd571f2bc2ad421cf6e6e88d938f8dcb5fd87b5af6033d6282/cache-47a051049e3398bb.arrow\n",
      "Loading cached split indices for dataset at /home/beast/.cache/huggingface/datasets/nlu_evaluation_data/default/1.1.0/0416a5876d8240bd571f2bc2ad421cf6e6e88d938f8dcb5fd87b5af6033d6282/cache-a9b8f3898ff3c07b.arrow and /home/beast/.cache/huggingface/datasets/nlu_evaluation_data/default/1.1.0/0416a5876d8240bd571f2bc2ad421cf6e6e88d938f8dcb5fd87b5af6033d6282/cache-d81a8b2ebbbe05fd.arrow\n"
     ]
    }
   ],
   "source": [
    "# Creating the splits\n",
    "train_val_datasets = dataset.train_test_split(test_size=5715, shuffle=True, seed=42)\n",
    "trainl_trainu_datasets = train_val_datasets['train'].train_test_split(test_size=19000, shuffle=True, seed=42)\n",
    "\n",
    "trainl_dataset = trainl_trainu_datasets['train']\n",
    "trainu_dataset = trainl_trainu_datasets['test']\n",
    "val_dataset = train_val_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7631bcbb-1dea-4db0-9101-8a757e1d2da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/beast/.cache/huggingface/datasets/nlu_evaluation_data/default/1.1.0/0416a5876d8240bd571f2bc2ad421cf6e6e88d938f8dcb5fd87b5af6033d6282/cache-da5008abbf4a1b94.arrow\n",
      "Loading cached processed dataset at /home/beast/.cache/huggingface/datasets/nlu_evaluation_data/default/1.1.0/0416a5876d8240bd571f2bc2ad421cf6e6e88d938f8dcb5fd87b5af6033d6282/cache-13de84dab523bc79.arrow\n",
      "Loading cached processed dataset at /home/beast/.cache/huggingface/datasets/nlu_evaluation_data/default/1.1.0/0416a5876d8240bd571f2bc2ad421cf6e6e88d938f8dcb5fd87b5af6033d6282/cache-032d0a35acf3dbd2.arrow\n",
      "Loading cached processed dataset at /home/beast/.cache/huggingface/datasets/nlu_evaluation_data/default/1.1.0/0416a5876d8240bd571f2bc2ad421cf6e6e88d938f8dcb5fd87b5af6033d6282/cache-61e5b0ebbafd87aa.arrow\n",
      "Loading cached processed dataset at /home/beast/.cache/huggingface/datasets/nlu_evaluation_data/default/1.1.0/0416a5876d8240bd571f2bc2ad421cf6e6e88d938f8dcb5fd87b5af6033d6282/cache-28da77e4bd5443bb.arrow\n",
      "Loading cached processed dataset at /home/beast/.cache/huggingface/datasets/nlu_evaluation_data/default/1.1.0/0416a5876d8240bd571f2bc2ad421cf6e6e88d938f8dcb5fd87b5af6033d6282/cache-229edfc16c79f447.arrow\n"
     ]
    }
   ],
   "source": [
    "# Tokenization, one-hot encoding, and formatting the get_item behavior\n",
    "processed_trainl_dataset = trainl_dataset.map(lambda examples: tokenizer(examples['text'], truncation=True, max_length=256, padding='max_length'), batched=True)\n",
    "processed_trainl_dataset = processed_trainl_dataset.map(lambda examples: {'labels': [1.0 if i == examples['label'] else 0.0 for i in range(dataset.features['label'].num_classes)]}, batched=False)\n",
    "processed_trainl_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "processed_trainu_dataset = trainu_dataset.map(lambda examples: tokenizer(examples['text'], truncation=True, max_length=256, padding='max_length'), batched=True)\n",
    "processed_trainu_dataset = processed_trainu_dataset.map(lambda examples: {'labels': [1.0 if i == examples['label'] else 0.0 for i in range(dataset.features['label'].num_classes)]}, batched=False)\n",
    "processed_trainu_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "processed_val_dataset = val_dataset.map(lambda examples: tokenizer(examples['text'], truncation=True, max_length=256, padding='max_length'), batched=True)\n",
    "processed_val_dataset = processed_val_dataset.map(lambda examples: {'labels': [1.0 if i == examples['label'] else 0.0 for i in range(dataset.features['label'].num_classes)]}, batched=False)\n",
    "processed_val_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cb9b10-993c-41e3-ac61-0677c8ca4533",
   "metadata": {},
   "source": [
    "## Creating an elastic indexer for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "944ab099-387f-45e4-b4e2-3015fdd98fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ES_JAVA_OPTS=\"-Xms2g -Xmx2g\" ./../elasticsearch-7.13.2/bin/elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c67f9a99-a5d9-4b3d-99a5-613171e1fa56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beast/code/fewl/venv/lib/python3.8/site-packages/elasticsearch/connection/base.py:208: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.13/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e916d2492c743998f144d484b17abcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['item_number', 'label', 'scenario', 'text'],\n",
       "    num_rows: 19000\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_client = elasticsearch.Elasticsearch()\n",
    "es_client.indices.delete(index='trainu', ignore=[400, 404])\n",
    "trainu_dataset.add_elasticsearch_index(column='text', es_client=es_client, es_index_name='trainu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f22b9b-2003-491e-9d94-f523cee43bf1",
   "metadata": {},
   "source": [
    "## Creating the MLM trained model\n",
    "We use the \"unlabeled\" data to train a model using Masked Language Modeling (MLM). We will use this base model later in some experiments. The training take quite a few hours of GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2253979-4f98-42ed-85ea-cf8ec465525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_model = AutoModelForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "mlm_data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "mlm_training_args = TrainingArguments(\n",
    "    output_dir=\"./models/nlu_evaluation_data/mlm\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy='epoch'\n",
    ")\n",
    "\n",
    "mlm_trainer = Trainer(\n",
    "    model=mlm_model,\n",
    "    args=mlm_training_args,\n",
    "    data_collator=mlm_data_collator,\n",
    "    train_dataset=processed_trainu_dataset,\n",
    "    eval_dataset=processed_val_dataset,\n",
    ")\n",
    "\n",
    "mlm_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbbc54c-705f-4d67-80dd-49f8585c48d7",
   "metadata": {},
   "source": [
    "## Training a classifier only on labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8da937-03d3-4c80-9e9c-364c6958c3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_config = AutoConfig.from_pretrained('bert-base-uncased', num_labels=dataset.features['label'].num_classes)\n",
    "classifier_model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', config=classifier_config)\n",
    "    \n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    accuracy = np.mean(np.argmax(p.predictions, axis=1) == np.argmax(p.label_ids, axis=1))\n",
    "    return {'accuracy': accuracy}\n",
    "    \n",
    "classifier_training_args = TrainingArguments(\n",
    "    output_dir='./models/nlu_evaluation_data/classifier',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=64,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy='epoch'\n",
    ")\n",
    "\n",
    "classifier_trainer = Trainer(\n",
    "    model=classifier_model,\n",
    "    args=classifier_training_args,\n",
    "    train_dataset=processed_trainl_dataset,\n",
    "    eval_dataset=processed_val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "classifier_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3c82a1-36d4-4ac8-bbc3-2507587c44b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training a classifier on labeled data on top of MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35982d2a-b386-4f28-836a-bb90dde04ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_mlm_checkpoint = str(sorted(list(pathlib.Path('.').glob('./models/nlu_evaluation_data/mlm/')))[-1])\n",
    "print(f'Uisng MLM checkpoint {last_mlm_checkpoint}')\n",
    "\n",
    "classifier_mlm_config = AutoConfig.from_pretrained('bert-base-uncased', num_labels=dataset.features['label'].num_classes)\n",
    "classifier_mlm_model = AutoModelForSequenceClassification.from_pretrained(last_mlm_checkpoint, config=classifier_mlm_config)\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    accuracy = np.mean(np.argmax(p.predictions, axis=1) == np.argmax(p.label_ids, axis=1))\n",
    "    return {'accuracy': accuracy}\n",
    "    \n",
    "classifier_mlm_training_args = TrainingArguments(\n",
    "    output_dir='./models/nlu_evaluation_data/classifier_on_mlm',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=64,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy='epoch'\n",
    ")\n",
    "\n",
    "classifier_mlm_trainer = Trainer(\n",
    "    model=classifier_mlm_model,\n",
    "    args=classifier_mlm_training_args,\n",
    "    train_dataset=processed_trainl_dataset,\n",
    "    eval_dataset=processed_val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "classifier_mlm_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e90f567-9839-4279-9f00-61fff9629a58",
   "metadata": {},
   "source": [
    "## Graph Agreement Model (GAM) -based training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ccc844-164f-4b36-8bd6-e6db3923d68f",
   "metadata": {},
   "source": [
    "### Pairing function\n",
    "We use the below function to create positive or negative pair for each datapoint. This pair will be used in training the agreement model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e901f2bf-0510-4d8e-a92a-39b49141627b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_up(batch, match):\n",
    "    \"\"\"\n",
    "    This function creates positive or negative pair for a given labeled batch\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'text': [],\n",
    "        'text_other': [],\n",
    "        'match': [],\n",
    "    }\n",
    "    label_to_indices = defaultdict(list)\n",
    "    for index, label in enumerate(batch['label']):\n",
    "        label_to_indices[label].append(index)\n",
    "    for label, text in zip(batch['label'], batch['text']):\n",
    "        if match == 'positive':\n",
    "            random_positive_index = random.choice(label_to_indices[label])\n",
    "            text_to_append = batch['text'][random_positive_index]\n",
    "            match_to_append = 1\n",
    "        elif match == 'negative':\n",
    "            labels_wihtout_this = [l for l in label_to_indices.keys() if label_to_indices[l] and l != label]\n",
    "            # labels_wihtout_this.remove(label)\n",
    "            random_negative_label = random.choice(labels_wihtout_this)\n",
    "            random_negative_index = random.choice(label_to_indices[random_negative_label])\n",
    "            text_to_append = batch['text'][random_negative_index]\n",
    "            match_to_append = 0\n",
    "        result['match'].append(match_to_append)\n",
    "        if random.choice([0, 1]):\n",
    "            # swap the texts \n",
    "            result['text'].append(text_to_append)\n",
    "            result['text_other'].append(text)\n",
    "        else:\n",
    "            result['text'].append(text)\n",
    "            result['text_other'].append(text_to_append)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831c7b9a-5dc6-4c94-a9a2-af24dfece5ed",
   "metadata": {},
   "source": [
    "### Create agreement training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58078c71-5a27-4be5-86b9-0df6d89d149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_dataset_positive = trainl_dataset.map(lambda examples: pair_up(examples, match='positive'), batched=True)\n",
    "paired_dataset_negative = trainl_dataset.map(lambda examples: pair_up(examples, match='negative'), batched=True)\n",
    "paired_dataset = concatenate_datasets([paired_dataset_positive, paired_dataset_negative])\n",
    "\n",
    "paired_dataset = paired_dataset.map(lambda examples: tokenizer(examples['text'], examples['text_other'], truncation=True, max_length=256, padding='max_length'), batched=True)\n",
    "paired_dataset = paired_dataset.map(lambda examples: {'labels': examples['match']}, batched=True)\n",
    "paired_train_val_datasets = paired_dataset.train_test_split(test_size=0.2, shuffle=True, seed=42)\n",
    "\n",
    "paired_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffb1ba5-1b43-4e59-b42f-234eb3ff7ace",
   "metadata": {},
   "source": [
    "### Train the agreement model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62075450-380b-4f9b-b7fc-07cada03d870",
   "metadata": {},
   "outputs": [],
   "source": [
    "agreement_model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    accuracy = np.mean(np.argmax(p.predictions, axis=1) == p.label_ids)\n",
    "    return {'accuracy': accuracy}\n",
    "    \n",
    "agreement_training_args = TrainingArguments(\n",
    "    output_dir='./models/nlu_evaluation_data/agreement',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy='epoch'\n",
    ")\n",
    "\n",
    "agreement_trainer = Trainer(\n",
    "    model=agreement_model,\n",
    "    args=agreement_training_args,\n",
    "    train_dataset=paired_train_val_datasets['train'],\n",
    "    eval_dataset=paired_train_val_datasets['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "agreement_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06547cd-33f1-4179-84df-fbf232a3b861",
   "metadata": {},
   "source": [
    "### Fish for new confident labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75c8afba-bfba-4f72-90ab-b38b4bd84b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]<ipython-input-28-53b023c0739e>:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  confidence = torch.nn.Softmax()(output['logits'][i]).detach().numpy()[1].item() # output['logits'][i][1].detach().numpy().item()\n",
      " 21%|██        | 206/1000 [02:13<07:44,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hackernews\n",
      "{'item_number': [], 'label': [], 'scenario': [], 'text': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 329/1000 [03:24<06:36,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "veganism\n",
      "{'item_number': [], 'label': [], 'scenario': [], 'text': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 831/1000 [08:19<01:38,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myspace\n",
      "{'item_number': [], 'label': [], 'scenario': [], 'text': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [09:58<00:00,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on positive examples: 0.7237237237237237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "agreement_model = AutoModelForSequenceClassification.from_pretrained('./models/nlu_evaluation_data/agreement/checkpoint-500')\n",
    "device = 'cpu'\n",
    "agreement_model = agreement_model.to(device)\n",
    "k = 5\n",
    "candidates = defaultdict(list)\n",
    "for datapoint_l in tqdm(trainl_dataset):\n",
    "    text_l = datapoint_l['text']\n",
    "    scores, retrieved_examples = trainu_dataset.get_nearest_examples(index_name='text', query=text_l, k=k)\n",
    "    retrieved_count = len(retrieved_examples['text'])\n",
    "    if retrieved_count == 0:\n",
    "        continue\n",
    "    tokenized_pairs = tokenizer([text_l] * retrieved_count, retrieved_examples['text'], truncation=True, max_length=256, padding='max_length')\n",
    "    batch = {k: torch.tensor(v).to(device) for k, v in tokenized_pairs.items()}\n",
    "    output = agreement_model(**batch)\n",
    "    for i in range(retrieved_count):\n",
    "        if output['logits'][i][1] > output['logits'][i][0]:  # positive pair\n",
    "            item_number = retrieved_examples['item_number'][i]\n",
    "            confidence = torch.nn.Softmax()(output['logits'][i]).detach().numpy()[1].item() # output['logits'][i][1].detach().numpy().item()\n",
    "            true_label = retrieved_examples['label'][i]\n",
    "            candidates[datapoint_l['label']].append((confidence, item_number, true_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0cc9c3e7-15fa-4737-99c0-c4f898c37698",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66/66 [00:00<00:00, 70170.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "           1       1.00      0.80      0.89         5\n",
      "           2       0.83      1.00      0.91         5\n",
      "           3       1.00      0.40      0.57         5\n",
      "           4       1.00      1.00      1.00         5\n",
      "           6       0.62      1.00      0.77         5\n",
      "           7       0.71      1.00      0.83         5\n",
      "           8       1.00      1.00      1.00         5\n",
      "           9       0.83      1.00      0.91         5\n",
      "          11       1.00      1.00      1.00         5\n",
      "          12       1.00      0.20      0.33         5\n",
      "          13       0.67      0.80      0.73         5\n",
      "          14       0.83      1.00      0.91         5\n",
      "          15       1.00      1.00      1.00         5\n",
      "          16       1.00      0.60      0.75         5\n",
      "          17       0.67      0.80      0.73         5\n",
      "          18       1.00      1.00      1.00         5\n",
      "          19       1.00      1.00      1.00         5\n",
      "          20       1.00      1.00      1.00         5\n",
      "          21       1.00      1.00      1.00         5\n",
      "          22       1.00      0.80      0.89         5\n",
      "          23       1.00      0.80      0.89         5\n",
      "          24       0.83      1.00      0.91         5\n",
      "          25       1.00      1.00      1.00         5\n",
      "          26       1.00      1.00      1.00         5\n",
      "          27       0.50      0.40      0.44         5\n",
      "          28       0.83      1.00      0.91         5\n",
      "          29       1.00      0.60      0.75         5\n",
      "          30       1.00      1.00      1.00         5\n",
      "          31       0.56      1.00      0.71         5\n",
      "          32       0.71      1.00      0.83         5\n",
      "          33       0.83      1.00      0.91         5\n",
      "          34       0.00      0.00      0.00         5\n",
      "          35       0.80      0.80      0.80         5\n",
      "          36       0.80      0.80      0.80         5\n",
      "          37       0.80      0.80      0.80         5\n",
      "          38       1.00      0.60      0.75         5\n",
      "          39       0.75      0.60      0.67         5\n",
      "          40       1.00      0.40      0.57         5\n",
      "          41       1.00      0.40      0.57         5\n",
      "          42       0.25      0.20      0.22         5\n",
      "          43       0.50      0.40      0.44         5\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.83      1.00      0.91         5\n",
      "          46       1.00      0.60      0.75         5\n",
      "          47       1.00      1.00      1.00         5\n",
      "          48       0.36      1.00      0.53         5\n",
      "          49       1.00      0.60      0.75         5\n",
      "          50       0.62      1.00      0.77         5\n",
      "          51       1.00      1.00      1.00         5\n",
      "          52       1.00      0.80      0.89         5\n",
      "          53       0.40      0.80      0.53         5\n",
      "          54       0.83      1.00      0.91         5\n",
      "          55       1.00      1.00      1.00         5\n",
      "          56       0.83      1.00      0.91         5\n",
      "          57       0.38      0.60      0.46         5\n",
      "          58       1.00      0.75      0.86         4\n",
      "          59       1.00      1.00      1.00         5\n",
      "          60       1.00      0.80      0.89         5\n",
      "          61       0.80      0.80      0.80         5\n",
      "          62       0.67      0.40      0.50         5\n",
      "          63       0.71      1.00      0.83         5\n",
      "          64       0.83      1.00      0.91         5\n",
      "          65       0.00      0.00      0.00         5\n",
      "          66       1.00      0.80      0.89         5\n",
      "          67       0.71      1.00      0.83         5\n",
      "\n",
      "    accuracy                           0.80       329\n",
      "   macro avg       0.82      0.80      0.79       329\n",
      "weighted avg       0.82      0.80      0.79       329\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/beast/code/fewl/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/beast/code/fewl/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/beast/code/fewl/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "for true_label, preds in tqdm(candidates.items()):\n",
    "    preds = sorted(preds, reverse=True)[0:5]\n",
    "    for pred in preds:\n",
    "        y_true.append(true_label)\n",
    "        y_pred.append(pred[2])\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "00f38f97-36cd-4ce5-a0e9-954876eb4a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf16095c3024b7b8f2e9d4f0a54331e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainu_dataset.drop_index('text')\n",
    "\n",
    "\n",
    "item_numbers = []\n",
    "for true_label in candidates.keys():\n",
    "    top_candidates = sorted(candidates[true_label], reverse=True)\n",
    "    for candidate in top_candidates[:best_per_class]:\n",
    "        item_numbers.append(candidate[1])\n",
    "\n",
    "# Add new items to trainl\n",
    "to_add_to_trainl_dataset = trainu_dataset.filter(lambda example: example['item_number'] in item_numbers)\n",
    "for datapoint in tqdm(to_add_to_trainl_dataset):\n",
    "    trainl_dataset = trainl_dataset.add_item(datapoint)\n",
    "\n",
    "# Remove these items now from trainu\n",
    "trainu_dataset = trainu_dataset.filter(lambda example: example['item_number'] not in item_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4aa6a0-7d6c-4329-ae4e-2bd41333fd28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
